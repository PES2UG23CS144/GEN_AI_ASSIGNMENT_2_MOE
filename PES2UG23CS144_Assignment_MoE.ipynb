{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Install dependencies\n",
        "!pip install -q groq\n",
        "\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "# 2. Setup API Key - REPLACE WITH YOUR ACTUAL KEY\n",
        "GROQ_API_KEY = \"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\"\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# 3. Define Expert Configurations\n",
        "# Using Llama 3.3 70B for complex tasks and 8B for simple/fast routing\n",
        "MODEL_CONFIG = {\n",
        "    \"technical\": {\n",
        "        \"system_prompt\": \"You are a Rigorous Technical Expert. Focus on code precision and debugging. Use Markdown for code blocks.\",\n",
        "        \"model\": \"llama-3.3-70b-versatile\"\n",
        "    },\n",
        "    \"billing\": {\n",
        "        \"system_prompt\": \"You are an Empathetic Billing Expert. Focus on financial policies and refund windows (3-5 days).\",\n",
        "        \"model\": \"llama-3.3-70b-versatile\"\n",
        "    },\n",
        "    \"general\": {\n",
        "        \"system_prompt\": \"You are a friendly General Expert for casual conversation.\",\n",
        "        \"model\": \"llama-3.1-8b-instant\"\n",
        "    },\n",
        "    \"tool_use\": {\n",
        "        \"system_prompt\": \"You are a Tool Specialist. Provide specific data points or mock API results.\",\n",
        "        \"model\": \"llama-3.1-8b-instant\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# 4. The Router Function (Gating Network)\n",
        "def route_prompt(user_input):\n",
        "    router_instruction = (\n",
        "        \"Classify this query into one of these categories: [technical, billing, general, tool_use]. \"\n",
        "        \"Return ONLY the word, nothing else.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": router_instruction},\n",
        "                {\"role\": \"user\", \"content\": user_input}\n",
        "            ],\n",
        "            model=\"llama-3.1-8b-instant\", # Fast model for routing\n",
        "            temperature=0 # Deterministic\n",
        "        )\n",
        "        return response.choices[0].message.content.strip().lower()\n",
        "    except Exception as e:\n",
        "        print(f\"Routing Error: {e}\")\n",
        "        return \"general\"\n",
        "\n",
        "# 5. Bonus: Mock Tool Function\n",
        "def mock_tool_expert(user_input):\n",
        "    if \"bitcoin\" in user_input.lower():\n",
        "        return \"The current mock price of Bitcoin is $98,450.25 (Simulated API Call).\"\n",
        "    return \"Tool Expert: Data not found for this specific request.\"\n",
        "\n",
        "# 6. The Orchestrator\n",
        "def process_request(user_input):\n",
        "    # Determine the expert\n",
        "    category = route_prompt(user_input)\n",
        "    print(f\"--- [Router Decided: {category.upper()}] ---\")\n",
        "\n",
        "    # Handle the Bonus Challenge (Tool Use)\n",
        "    if \"tool_use\" in category:\n",
        "        return mock_tool_expert(user_input)\n",
        "\n",
        "    # Select configuration\n",
        "    config = MODEL_CONFIG.get(category, MODEL_CONFIG[\"general\"])\n",
        "\n",
        "    # Call the selected Expert\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": config[\"system_prompt\"]},\n",
        "                {\"role\": \"user\", \"content\": user_input}\n",
        "            ],\n",
        "            model=config[\"model\"],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Expert Error: {e}\"\n",
        "\n",
        "# 7. Execution Loop\n",
        "test_queries = [\n",
        "    \"My python script is throwing an IndexError on line 5.\",\n",
        "    \"I was charged twice for my subscription this month.\",\n",
        "    \"Hey! How's your day going?\",\n",
        "    \"What is the current price of Bitcoin?\"\n",
        "]\n",
        "\n",
        "print(\"Starting MoE Router System...\\n\")\n",
        "for query in test_queries:\n",
        "    print(f\"User Input: {query}\")\n",
        "    result = process_request(query)\n",
        "    print(f\"Final Response:\\n{result}\")\n",
        "    print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOzTFIwa8OlT",
        "outputId": "7c83d01b-2f76-4032-9821-2effb5ffa338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/138.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRpVYHz6AbEB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}